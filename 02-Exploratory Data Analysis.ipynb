{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2d26f2",
   "metadata": {},
   "source": [
    "<img src='./fig/vertical_COMILLAS_COLOR.jpg' style= 'width:70mm'>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "Máster en Big Data. Tecnología y Analítica Avanzada (MBD).\n",
    "</h1>\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "Introducción al Análisis Estadístico con Lenguajes de Programación para Machine Learning (IAELPML). 2023-2024.\n",
    "</h1>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "02 Exploratory Data Analysis\n",
    "<a class=\"tocSkip\">    \n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c678edc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style='background:yellow; color:red'> Remember:<a class=\"tocSkip\"> </span>     \n",
    "\n",
    "+ Navigate to your `IAELPML` folder in the console  \n",
    "+ Execute `git pull origin main` to update the code\n",
    "+ **Do not modify the files in that folder**, copy them elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26924db1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with Data Files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fb34f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Data Files (txt, csv) with pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd8ca8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ We will begin by downloading one of these files, called `movies.csv` containing information about the top watched films (in theaters) between 2007 and 2011. \n",
    "\n",
    "+ We strongly recommend that you always create a project specific `data` folder and store all the downloaded data files for each project in there. In our case you can keep this folder as a subfolder of the *Course Folder*. In particular, do not place the files inside the *Common Repository folder* (the one named *IAELPML*). \n",
    "\n",
    "+ Use the following link to download the data file (it is advised to use the right button of your mouse and choose an option such as \"download the linked file as...\"):  \n",
    "\n",
    "$\\qquad$[movies.csv](https://gist.githubusercontent.com/tiangechen/b68782efa49a16edaf07dc2cdaa855ea/raw/0c794a9717f18b094eabab2cd6a6b9a226903577/movies.csv)\n",
    "\n",
    "+ Before proceeding to read the file with Python **you should always begin examining the contents of a data file with a text editor** (such as `Notepad++` in Windows or BBedit in Mac). Not doing so will often result in bad data processing downstream.\n",
    "\n",
    "+ In this case opening the file in a text editor (BBedit in the Mac here) shows something like this (only the first few lines are shown):\n",
    "\n",
    "<img src='./fig/010-movies_csv_open_text_editor.png' style= 'width:300mm'>\n",
    "\n",
    "\n",
    "+ There are some key ingredients of such files that you need to pay attention to here, even in simple cases like this first example:\n",
    "    + Each line of the file corresponds to a row of the data table. \n",
    "    + The first line or *header* is often a special line, containing the names of the table columns. In the best case scenario these correspond to the variables in our data.\n",
    "    + The individual column values in every line are separated by a fixed symbol: a comma, a semicolon, a space or tab, etc. This will be called the *separator* and it is critical to determine which one was used in each data file. For movies.csv the separator is a comma.\n",
    "    \n",
    "+ Now that we have some information about the file we are ready to load it into a pandas Dataframe, that we will also call `movies`. Note that we first need to import pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c5dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('data/movies.csv', sep=\",\", header=0)\n",
    "\n",
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268801e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "+ The arguments `sep` and `header` provide the pandas [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function with the information we discovered in our initial file exploration. If you run the function like this:\n",
    "\n",
    "    ```python\n",
    "    movies = pd.read_csv('data/movies.csv')\n",
    "    ```\n",
    "    pandas will try to *guess* which separator to use and whether or not there is a header line. This `movies.csv` file is such an easy example that the code will work either way. But be warned: this will sometimes fail and when working with many large files the guessing adds an unnecessary overhead to the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528bdea",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "+ using the commands we saw in `01a_test_your_setup` find out the shape (rows, columns) of this DataFrame.\n",
    "+ use the bracket selection (with `loc` or `iloc`) to get the `Genre` information for the movies from year 2010. What type of object did you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S02-001.py\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33d50d",
   "metadata": {},
   "source": [
    "### Reading the csv file from a URL (link)\n",
    "\n",
    "+ Sometimes it is simpler to skip the download part of the process and read the data directly from a link. Keep in mind, however, that for this to work the same caveats as before apply: it is better to explicitly set the separator, to know beforehand if there is a header in the file, etc.\n",
    "\n",
    "+ Let us see a classic example: the `mpg` data set can be downloaded [in csv format with this link](https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/mpg.csv). This data set contains information about fuel consumption (miles per gallon, thus mpg) and other characteristics of some car models from 1999 to 2008. You can read more aboout it and the variables it contains [in this link](https://ggplot2.tidyverse.org/reference/mpg.html).\n",
    "\n",
    "+ We use this example to show that we can use the link to the csv file as argument for pandas `read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48949b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_link = \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/mpg.csv\"\n",
    "mpg = pd.read_csv(mpg_link)\n",
    "\n",
    "print(mpg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fd5ce",
   "metadata": {},
   "source": [
    "### Creating and saving a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19def427",
   "metadata": {},
   "source": [
    "+ Next let us begin by creating a pandas DataFrame with synthetic data. Learning to create such objects is an essential part of the data analysis skills required in this course. You will often need to simulate data with prescribed stratistical properties in order to perform computer experiments. These are in turn the key to understand the expected behavior in many real world problems. \n",
    "\n",
    "+ The NumPy and Scipy modules offer many tools for statistical modeling, and we will be meeting many of these tools in the next sessions. Here we begin with a simple example in which we use NumPy. You can [use this link](https://numpy.org/doc/1.16/reference/routines.random.html) to read the documentation for the functions we use and to get a first idea of the possibilities. For the time being it is ok if you don't fully understand some of the words below. We will discuss these issues later in the course:\n",
    "    + The `np.random.randint` function to generate 100 random integers between 1 and 20 (both included, of course with replacement). These become the pandas Series `A`. Recall that each column of a Data.Frame is a Series.\n",
    "    + The `str.ascii_lowercase` string contains all the English alphabet lower case letters. We use a list comprehension to convert that to a NumPy array and then use `np.random.choice` to select 100 random letters with replacement. These form the Series `B`.\n",
    "    + Finally we use `np.random.random_sample` to obtain 100 random floating values uniformly distributed in the $[-4, 4]$ interval. These form the Series `C`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2022)  # seed for reproducibility\n",
    "\n",
    "# 100 random integers\n",
    "A = pd.Series(np.random.randint(low = 1, high = 7, size = 100))\n",
    "\n",
    "# 100 random lowercase letters\n",
    "import string as str\n",
    "alphabet_string = str.ascii_lowercase\n",
    "alphabet = np.array([alphabet_string[i] for i in range(len(alphabet_string))])\n",
    "B = pd.Series(np.random.choice(alphabet, size=100))\n",
    "\n",
    "# 100 random floats in [-4, 4]\n",
    "C = pd.Series(8 * np.random.random_sample(size=100) - 4)\n",
    "\n",
    "\n",
    "# Use the above as columns for a DataFrame\n",
    "X = pd.DataFrame({'A':A, 'B':B, 'C':C})\n",
    "print(X.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994423d",
   "metadata": {},
   "source": [
    "+ Now that we have created this DataFrame we can save it to a csv file. To identify it we will name that file `EDA_data_saving.csv` (in the `data` folder). Every pandas DataFrame has a `to_csv` method that we can use for this. Run the following command and then use a text editor (Notepad++, BBEdit or similar) to check that the contents are what you expect. Make sure to check the header line, the separator and the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"data/EDA_data_saving.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be997c",
   "metadata": {},
   "source": [
    "+ **Exercise:** change the separator to a comma and set `index = True`. Then run the code and check the changes. \n",
    "\n",
    "    In general we recommend setting `index=False` unless you have a good reason not to. These makes reading the csv files in other programs easier. Be careful because the default behavior is `index=True`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S02-002.py\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1c503",
   "metadata": {},
   "source": [
    "## Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe52ed",
   "metadata": {},
   "source": [
    "+ Excel files are frequently used as a data source in some contexts. For simple files, reading their content into a pandas DataFrame is quite straightforward. But for more complex cases things can get more complicated. However, to keep things simple here we will use a simple file containing data about 2010 train accidents in the US. You can download the file (to the data folder) using [right-click in this link](http://users.stat.ufl.edu/~winner/data/train_acc_2010.xls). And the meaning of the variables in this dataset is [clarified by this link](https://users.stat.ufl.edu/~winner/data/train_acc_2010.txt)\n",
    "\n",
    "\n",
    "+ First we need to install an additional module into the `IAELPML` environment. This will often be the case when reading data from other sources. Do you remember how to install new modules?  \n",
    "**Exercise:** install the module called `xlrd` into the `IAELPML` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3006106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S02-003.py\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374b5c1",
   "metadata": {},
   "source": [
    "+ Then run this code to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85036b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = './data/train_acc_2010.xls'\n",
    "\n",
    "train_accidents = pd.read_excel(xlsx_file)\n",
    "\n",
    "print(train_accidents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de66a4",
   "metadata": {},
   "source": [
    "+ You can read the [documentation for the `read_excel` function here](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html). The examples therein will show you how to select a particular range of rows and columns in the spreadsheet and how to deal with the case when there are several sheets of data in the Excel file. And if you need to export directly from Python to Excel [read about the `to_excel` method here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae346c5",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Data Files from Other Programs\n",
    "\n",
    "+ Eventually you may find yourself trying to import data stored in the file format used by some statistical software. In such cases, *the Web is your friend*. Look for help for importing that specific type of file into a pandas DataFrame. In fact, let us do this right now as an exercise:  \n",
    "    **Exercise:** the [`auto2.dta` file in this link](http://www.stata-press.com/data/r15/auto2.dta) is a data file stored in *Stata* format. Download the file to your data folder and look for a way to import this into a pandas DataFrame (call it `auto2` as well, we will use it later). **Warning:** Some of the cells below will not work until you complete these exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2aa40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S02-004.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047182e4",
   "metadata": {},
   "source": [
    "# Different Types of Data\n",
    "\n",
    "+ The data sets we are beginning to see in these examples follow the two most basic rules of what is called *Tidy Data* (a term coined by Hadley Wickham in 2014):\n",
    "    + Each column of the table corresponds to a *variable*.\n",
    "    + Each row corresponds to an *observation*, *individual*, *case* or *sample*. All these terms refer to a single observational unit of whatever it is that we are considering.\n",
    "    \n",
    "+ The variables can be of different types: integer or floating point numbers, dates, binary variables (yes/no type), text strings, labels, dates, positions, etc. Those different types of variables allow for different operations and need specific treatment in many cases, if we are to get out of them as much information as possible. \n",
    "\n",
    "+ In the next sections we will meet the basic types of data (variables) and the best way to describe them. We will be using some of the examples that we have already seen and some others like the ones introduced here:\n",
    "\n",
    "    + The framingham data set contains health data from a heart disease study that took place in Framingham (UK). [This link contains the csv file URL](https://raw.githubusercontent.com/mbdfmad/fmad2122/main/data/framingham.csv) and you can [read more about the study and the variables in the table here](https://biolincc.nhlbi.nih.gov/media/teachingstudies/FHS_Teaching_Longitudinal_Data_Documentation.pdf?link_time=2019-08-26_14:42:24.487245).\n",
    "    + The `flights` data set contains information about flights that departed from New York in 2013. [This link contains the csv file URL](https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv). You can read more about this data set [in this link](https://rdrr.io/cran/nycflights13/man/flights.html).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9f2a8",
   "metadata": {},
   "source": [
    "+ **Exercise:** using the URLs above directly (without downloading) load these data sets into two pandas DataFrames called, respectively, `framingham` and `flights`.   **Warning:** Some of the cells below will not work until you complete these exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2117b",
   "metadata": {
    "hide_input": false,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# %load \"./exclude/S02-005.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb663f",
   "metadata": {
    "hide_input": false,
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "print(framingham.columns)\n",
    "print(framingham.head())\n",
    "print(framingham.describe())\n",
    "print(framingham.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b98993",
   "metadata": {
    "scrolled": false,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "print(flights.columns)\n",
    "print(flights.head())\n",
    "print(flights.describe())\n",
    "print(flights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c1bd0",
   "metadata": {},
   "source": [
    "## The `info` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c542c8",
   "metadata": {},
   "source": [
    "+ When we start exploring a new data set one of the first things we should do, is find out the size of the table (e.g. using the `shape` method) and **what types of data it contains**. The `info` method from `pandas` is usually a good starting point. Let us see how it works with the `train_accidents` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca25bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accidents.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6b1b9",
   "metadata": {},
   "source": [
    "+ The output shows that the numeric variables in this table have all been read as integers (`Dtype int64`) while the variables whose values are character strings have been read as `Dtype object`. When reading data from other files and sources we will come across other Python data types (e.g. floats). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f065b",
   "metadata": {},
   "source": [
    "+ **Exercise:** apply the `info` method to the following data sets:\n",
    "    + `movies`\n",
    "    + `X`\n",
    "    + `framingham`\n",
    "    + `flights`\n",
    "    \n",
    "    Look at the results and think about the variables in those data sets. How many different kinds of data can you find? Can you think of another types of data not present in those examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd056b99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load \"./exclude/S02-006.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cbd05",
   "metadata": {},
   "source": [
    "## Preliminary Classification of Variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d9600",
   "metadata": {},
   "source": [
    "+ Let us pause here to think about the kind of data types in the examples we have seen. Many (but not all) of the variables / columns in the data tables fit into one of these categories:\n",
    "    + **Quantitative or Numeric Variables**. That means that the values of these variables are *numbers that have been measured in a meaningful scale, such that the operations with this numbers have meaning*. Numeric variables are further divided into **Discrete** and **Continuous** variables. We will discuss this with detail below.\n",
    "    + **Qualitative Variables or Factors**. The values of these variables are *labels* and they are used to *group the observations into classes*. A typical example would be grouping bird observations into species. The name of a species is a label, but it makes no sense to sum (or take the mean of) an ostrich and a sparrow. Even if the labels are sometimes numeric, operations have no sensible interpretation in relation with the data (we will see however an exception for binary variables when discussing proportions).  \n",
    "\n",
    "+ A numeric variable is **discrete** if there is a minimal or atomic unit of measurement such that all values of the variable are multiples of that unit. For example, when counting the coins in your wallet it makes no sense to say you have 3.5 coins. You may have 3 or 4, but no \"fraction of a coin\" counts as a coin.\n",
    "\n",
    "+ On the other hand a numeric variable is **continuous** if its values are measured in a floating point (decimal) scale, such that given two values then any intermediate values is, in principle, a possible value for the variable. \n",
    "\n",
    "+ We will see, however, that the division between discrete and continuous is not always as clear cut as it may appear at first. At the end of the day, it is usually a modeling choice. We will see many examples of why this happens. \n",
    "\n",
    "+ **Exercise / Discussion.** Look at the variables in some of the tables from the previous examples. How do they fit this types of variables? Are they quantitative or are they factors? In the case of numeric variables, do you consider them continuous or discrete?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecfbaa",
   "metadata": {},
   "source": [
    "# Discrete (Quantitative) Variables\n",
    "\n",
    "## Frequency Tables\n",
    "\n",
    "+ Let us look at the synthetic `X` DataFrame that we created before. The `A` variable is one example of a discrete numeric variable, as it only takes integer values (and few different values; more on this later). The first step in the exploration of a discrete variable is to obtain an **absolute frequency table**. For each value of the variable, this table counts the number of appearances of that value in the table. In pandas you can get it using the `crosstab` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsFreq_X = X['A'].value_counts()\n",
    "AbsFreq_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b52c5",
   "metadata": {},
   "source": [
    "+ Note that the values in the table are not sorted by the size (they are jsut ordered by their order of appearance in the table). We will often want them ordered, especially in this case case of numeric variables. To get this ordered frequency table use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsFreq_X = X['A'].value_counts().sort_index()\n",
    "AbsFreq_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210bdd4",
   "metadata": {},
   "source": [
    "+ Absolute frequencies are often simply called *counts*. Of course the sum of the absolute frequencies equals the numer of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(AbsFreq_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d66e94",
   "metadata": {},
   "source": [
    "+ Absolute frequencies depend on the size of the whole data set. Sometimes, and specially when comparing two data sets, it is better to use *relative frequencies*. These are the answer to questions of like \"*What proportion of the values are equal to...?*\" In pandas you can easily modify `value_counts` to get the relative frequencies or proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "RelFreq_X = X['A'].value_counts(normalize=True).sort_index()\n",
    "RelFreq_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a13309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(RelFreq_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d1e289",
   "metadata": {},
   "source": [
    "### Some comments about relative frequencies\n",
    "\n",
    "+ Relative frequencies always add up to one. They are closely related to percents, but instead of using 100 as the total, we are using 1. \n",
    "+ More importantly, *relative frequencies are deeply connected with the first intuitive ideas about probability*. That is: imagine that you were to pick at random a row of the `X` table. What is the probability that the value of `A` in that row is 4. The relative frequency table tells us that we expect the value 4 to appear in 12 out of every 100 repetitions of that experiment.  \n",
    "\n",
    "### Cumulative frequencies (absolute and relative)\n",
    "\n",
    "+ Cumulative frequencies are used to respond to questions like \"how many values are lower than ...?\" os the analogous question in terms of proportions. We can easily obtain cumulative results using the `cumsum` method. Let us use it to get cumulative frequencies for `X`, both absolute and relative: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CumAbsFreq_X = AbsFreq_X.cumsum()\n",
    "CumAbsFreq_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da84bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CumRelFreq_X = RelFreq_X.cumsum()\n",
    "CumRelFreq_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02aa98e",
   "metadata": {},
   "source": [
    "+ The final cumulative frequency is always the total number of rows in the data set (in the absolute case) or 1 (in the relative case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040d10f",
   "metadata": {},
   "source": [
    "## Plots for discrete variables\n",
    "\n",
    "+ The basic and recommended graphical representation for a discrete variable is the *bar plot*. We will illustrate this with the `A` variable of `X`. We also import `seaborn` and use it first to set a nice plotting style. You can comment out the first two lines and rerun this notebook to see how the plot changes. But remember to import `seaborn` afterwards, as we will need it in some cells below this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "plot = AbsFreq_X.plot.bar(rot=0, xlabel='A', ylabel ='Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca3dfd",
   "metadata": {},
   "source": [
    "$\\quad$ The only difference when plotting the relative frequencies is the scale in the vertical axis.\n",
    "    \n",
    "+ The bar plot in this example gives us a visual summary of the *shape* of the data. In other words, we can see how the data is distributed among the different possible values. We will talk a lot about *distributions* in this course, and you can consider this humble bar plot as the first step in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plot = X['A'].value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbf934",
   "metadata": {},
   "source": [
    "# Continuous (Quantitative) Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd9e5e",
   "metadata": {},
   "source": [
    "+ We will first consider variables that are clearly treated as continuous in the data set. For example, the `C` variable in `X` was designed that way. For such a variable, frequency tables make no sense. Most values of such variables will appear only once in the table, making almost every absolute frequency equal to 1. We need a different set of tools for these variables.\n",
    "\n",
    "+ The simplest idea is to reduce the continuous variables to discrete ones using `binning`. That is we divide all  values of the variable into *bins*, which are a collection of intervals that span all possible values. In our present example, the variable `C` was designed to take random values in the $[-4, 4]$ interval. Thus we will divide that interval into a collection of subintervals or bins. How many bins? Well, that is not a simple question, because the answer depends on both the specific data and what you are trying to accomplish. A rule of thumb is not to use less than five bins and no more than $\\sqrt{n}$, where $n$ is the number of data points for your variable. In our case we will divide $[-4, 4]$ into eight subintervals or bins and we will assign each value of `C` to the corresponding subinterval. We can do that with the pandas cut function. We will first apply it naïvely to the variable and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31231436",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(X['C'], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5515be7",
   "metadata": {},
   "source": [
    "As you can see, the result is a Panda Series that contains the subinterval assigned to the corresponding values in `C`. But the endpoints of these intervals depend too much on the specifics of our data. Remember that these were randomly chosen in the $[-4, 4]$ interval. If we were to choose another sample of random points in the same interval, these endpoints probably wouldn't do. Thus we go back to `cut` but this time we explicitly set the endpoints of the bins (we call them *nodes* here), as a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_nodes = np.arange(-4, 5, 1)\n",
    "print(C_nodes)\n",
    "\n",
    "pd.cut(X['C'], bins=C_nodes, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506efff7",
   "metadata": {},
   "source": [
    "That looks much better. Note that we also took the chance to instruct Python to keep the subintervals ordered. In fact, the 'C_binned' variable is now an *ordered factor*, a concept that we will discuss below. We can now add that column to the DataFrame in a very simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['C_binned'] = pd.cut(X['C'], bins=C_nodes, ordered=True)\n",
    "print(X.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872d347",
   "metadata": {},
   "source": [
    "This `C_binned` variable is a factor (a qualitative variable), because its values are not numbers, but labels (the names of the subintervals). We will discuss factors below, but for now suffice it to say that we can also use the concept of frequencies with factors. We get this table (again, the order is taken into account):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130621f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['C_binned'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = X['C_binned'].value_counts().sort_index().plot.bar(rot=0, xlabel='A', ylabel ='Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daedf40",
   "metadata": {},
   "source": [
    "## Histograms and Density Plots\n",
    "\n",
    "\n",
    "The preceding plot is not bad, but it has some issues. The most relevant one is the fact that the *gaps* between the bars have lost utility. The division we made with cut created *consecutive intervals* qith no space between them. And this plot fails to convey that. The alternative is a classic type of plot for continuous data called the histogram. We can get it with pandas method `hist`. Here we have tweaked the result a little to improve the resulting plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = X.hist('C', edgecolor='black', linewidth=1.2, grid=False, color = \"tan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63648965",
   "metadata": {},
   "source": [
    "+ But you can notice here that `hist` has chosen the number of bins for us. It is often better to do this by hand. We wil use the `C_nodes` values again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bf60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = X.hist('C', edgecolor='black', linewidth=1.2, grid=False, color = \"tan\", bins=C_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ddb34",
   "metadata": {},
   "source": [
    "+ Please compare the histogram with the bar plot at the end of the previous section. And remember that instead of an explicit list of nodes, you can just give the number of bins or even let Python choose for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = X.hist('C', edgecolor='black', linewidth=1.2, grid=False, color = \"tan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94b026",
   "metadata": {},
   "source": [
    "## Discrete - Continuous Transition\n",
    "+ Frequency tables, the common idea behind bar plots and histograms, become soon useless as the number of different values of a variable increases. For example, if we look at the `CarsDer` variable in the `train_accidents` DataFrame (indicates the number of derailed train cars in the accident) and we ask Python for a frequency table, we will get this answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accidents['CarsDer'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98a812",
   "metadata": {},
   "source": [
    "**Exercise:** What is the cumulative relative frequency of 10 in the previous table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7480a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S02-007.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb28a3f",
   "metadata": {},
   "source": [
    "\n",
    "+ This is a common situation and it illustrates the kind of situation where the boundary between discrete and continuous variables becomes a blur. Not only the frequency table is too big to be useful. There is also the fact that the information it provides is, in this format, less and less relevant. Think about this: does it make much of a difference if the number of derailed cars was 27 instead of 26? And that kind of problem gets only worse as the number of different values increases. At some point, the information that becomes relevant is not the particular value, but the interval it belongs to. The questions we care about are *binned questions*, such as: did the accident involve more than 10 but less than 20 cars? This gives us a better picture of the accident's severity, possible cost and damages, etc. We will come back to this discussion later, when we talk about discrete and random continuous variables and their distributions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3692c40",
   "metadata": {},
   "source": [
    "## Density Curves\n",
    "\n",
    "+ A density curve is a graphical representation of the distribution of the values of a continuous variable. You can think of it as an alternative to the histogram that does not require binning. The density curve is best used when we have a large sample of values of the variable. Instead of using bars, it uses a curve. In the case of the `C` variable in `X` we can draw the density curve like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33060e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getPlot = X['C'].plot.density(linewidth=2, grid=False, color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e932d35",
   "metadata": {},
   "source": [
    "   **Exercise:** using the data in the Stata file `auto2.dta` that you imported before, plot density curves for the variables `price`, `trunk`, `weight` and `gear_ratio` in that data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488edd",
   "metadata": {},
   "source": [
    "+ Sometimes we will want to combine the histogram and the density plot. We can do this with `seaborn`, adding `kde=True` to a histogram plot. The last two commands control the aesthetics of the density curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_plot = sns.histplot(data = X, x = 'C', kde=True)\n",
    "dens_plot.lines[0].set_color('red')\n",
    "dens_plot.lines[0].set_linewidth(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd0cb3",
   "metadata": {},
   "source": [
    "+ Note that the vertical axis is `count`, indicating frequencies.  \n",
    "    **Exercise:** Add the argument `stat='density'` to the sns.histplot function and see what changes in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8f75f",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265ff49",
   "metadata": {},
   "source": [
    "+ In a previous exercise we asked you to plot the density curves for the variables the variables `price`, `trunk`, `weight` and `gear_ratio` in the `auto2` data set. If you have done the exercise you will have found curves like these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000316bf",
   "metadata": {
    "hide_input": false,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "auto2.head()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 7]\n",
    "densities = auto2.loc[:, ['price', 'trunk', 'weight', 'gear_ratio']].plot.density(\n",
    "    subplots=True, layout=(2,2), sharex=False,sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d0ee3",
   "metadata": {},
   "source": [
    "   Each one of these curves shows a **distribution** of values. We can think of a distribution in graphical terms as a *shape* of the data. The notion of distribution is central to Statistics and *Data Science* and we will spend part of our time learning how use them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4f98e",
   "metadata": {},
   "source": [
    "## Unimodal and Multimodal Distributions\n",
    "\n",
    "+ A distribution with a single maximum (or peak), like the one below, is called a **unimodal** distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2874e",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(0, 20, 0.001)\n",
    "getPlot = plt.plot(x, stats.chi2.pdf(x, df=4), color='red', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f513b8af",
   "metadata": {},
   "source": [
    "+ The density curve of a **multimodal** distribution on the other hand has more than one peak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e433d9e",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "mu1 = 12\n",
    "sigma1 = 0.7\n",
    "mu2 = 14\n",
    "sigma2 = 0.5\n",
    "W1 = np.random.normal(loc = mu1, scale = sigma1, size = N)\n",
    "W2 = np.random.normal(loc = mu2, scale = sigma2, size = N)\n",
    "W = np.concatenate((W1, W2), axis = None)\n",
    "W.shape\n",
    "X2 = pd.DataFrame({'W':W})\n",
    "getPlot = X2['W'].plot.density(linewidth=2, color = \"red\")\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e048e8e",
   "metadata": {},
   "source": [
    "## Symmetry and Skew\n",
    "\n",
    "+ Symmetry is also an interesting characteristic of a distribution shape. The longest **tail** (left and right) of the distribution determines the name we use to describe it. A **right skewed** distribution is one with a long right tail, as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6c19b",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "x = np.arange(0, 16, 0.001)\n",
    "plt.plot(x, stats.chi2.pdf(x, df=6), color='red', linewidth=3, )\n",
    "getPlot = plt.title('Right Skewed', fontdict={'fontsize': 25, 'color':'red'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84baedfd",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-3, 3, 0.001)\n",
    "plt.plot(x, stats.norm.pdf(x), color='blue', linewidth=3, )\n",
    "getPlot = plt.title('Symmetric', fontdict={'fontsize': 25, 'color':'blue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995abdb",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 16, 0.001)\n",
    "plt.plot(x, stats.chi2.pdf(16 - x, df=6), color='red', linewidth=3, )\n",
    "getPlot = plt.title('Left Skewed', fontdict={'fontsize': 25, 'color':'red'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77524eb",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e2cf6df",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Distributions and Probability: an Informal Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f9a0e",
   "metadata": {},
   "source": [
    "+ A histogram is a good view to visualize a continuous variable or, as we discussed before, a discrete variable with so many different values that the difference between them becomes irrelevant. In this latter case our interest shifts from individual values to intervals. \n",
    "+ A frequent question is: \"If we sample random values from a data set with a given distribution, what is the probability that the sampled value will belong to a certain interval?\"\n",
    "+ For example, the histogram below illustrates the distribution of a discrete variable whose values range from 0 to 20. The smallest values appear with much less frequency than those close to 14, and that is why the height of the first histogram bars is almost zero. In terms of probability, if we were to pick a random value of this distribution, the most probable values would be those close to 14. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b4d88",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "X3_nodes = np.arange(0, 21, 1)\n",
    "X3 = np.random.binomial(n=20, p=0.7, size=10000)\n",
    "getPlot = plt.hist(X3, bins=X3_nodes, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9364d9e",
   "metadata": {},
   "source": [
    "+ With the right choice of vertical scale the area of each bar in a histogram can be made to represent the relative frequency (that is, probability) of the corresponding value. Then the probability of a random value belonging to interval $[a, b]$ will be the sum of the areas of the bars in that interval. This is illustrated in the right side \n",
    "of the picture below.\n",
    "+ On the other hand, for a continuous variable the distribution is described by its density curve. And then the probability of a random value belonging to $[a, b]$ is given by the *integral*, that is the area, of the density curve over that interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f303f6b",
   "metadata": {},
   "source": [
    "![](fig/02-fig04-ProbabilidadIntervaloVariableDiscreta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c5918",
   "metadata": {},
   "source": [
    "+ **Do not panic!** \n",
    "![Panic](https://media.giphy.com/media/1EghTrigJJhq8/giphy.gif)\n",
    "You won't need to compute integrals (by hand) in this course. But ii is important to understand this concept: probabilities are tightly linked with areas and areas are strongly connected with integrals. That is why there is a lot of integrals underpinning the mathematical foundations of Statistics and Data Science. But whenever we need them, the computer will do the hard work for us via Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fb6bd",
   "metadata": {},
   "source": [
    "# Measures of Center and Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b71bd8",
   "metadata": {},
   "source": [
    "## The arithmetic mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde65503",
   "metadata": {},
   "source": [
    "+ When working with a quantitative (numeric) variable we often try to pick a *center*, *mean* or *central value* that can act as a good representative of the typical values of the variable. A good representative of the data could be, e.g. the answer to the question: \"if I were to pick a value of this variable at random, what do I expect it to be like?\"\n",
    "\n",
    "+ There are many ways of defining such central values. Each definition has its pros and cons. Here we will discuss some of the more useful ones.\n",
    "\n",
    "+ Starting with the *arithmetic mean*. If we have a sample of $n$ values of a numeric variable $X$, say:\n",
    "$$x_1, x_2, \\ldots, x_n$$\n",
    "then their arithmetic mean is defined by:\n",
    "$$\\bar X = \\dfrac{x_1 + x_2 + \\cdots + x_n}{n} = \\dfrac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "+ For a first example let us ask NumPy to pick 50 random numbers between 0 y 100 (with replacement) and find out their arithmetic mean. What do you expect to see as result? For reproducibility we first set the random seed generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92606dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rng.choice(101, 50, replace=True)\n",
    "x\n",
    "x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e91396",
   "metadata": {},
   "source": [
    "  Which is of course equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ded08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum() / x.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a238df",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "+ **Exercise (very important):** Execute the previous cells (excluding the random seed) and see what results you get. Better still, do that many times (thousands of times) inside a loop or list comprehension and study the different means that you get.  **Understanding the results of this exercise is a huge step in Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec7f8d",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "sample_means = pd.DataFrame([rng.choice(101, 50, replace=True).mean() for item in range(10000)], \n",
    "                            columns=['x'])\n",
    "sample_means.head(10)\n",
    "getPlot = sample_means.plot.density()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c21fe",
   "metadata": {},
   "source": [
    "+ The arithmetic mean is very popular because it is very easy to compute. The mathematics behind it are simple and have many good properties. But it has a major limitation: it is severely affected by the presence of *outliers* in the data. That term describes values which are (in absolute value) abnormally big when compared with the majority of the other values. \n",
    "\n",
    "+ Let us look at an example. Now we will consider 100 values. But 99 of them will be random integers between 0 and 101, and the last one will be 1000. If you ran the following cell a number of times you will notice that a single value (out of 100) shifts the mean upwards by roughly 10 units. In particular, that means the in this second case the mean fails to provide a good estimation of the typical size of a value picked at random from this sample. We often describe this situation saying that the arithmetic mean is not **robust** in the presence of outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323027c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(rng.choice(101, 99, replace=True))\n",
    "print(\"Mean of x before appending = \", x.mean())\n",
    "x = np.append(x, 1000)\n",
    "print(\"Tail of x after appending = \", x[95:])\n",
    "print(\"Mean of x after appending = \", x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b911e4d",
   "metadata": {},
   "source": [
    "## The median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be421f",
   "metadata": {},
   "source": [
    "+ The *median* aims to provide a robust representative of a numeric variable, one which is almost unaffected by the presence of outliers in our data. To get the median we sort our data and take the value in the middle of the ordered data. If the number of data values is odd then that is enough, but if it even then there is no value *in the middle*. In that case we take the median to be arithmetic mean of the largest and smallest value of the left and right halves of the data, respectively. \n",
    "\n",
    "+ Let us look at an example. Run the following two cells in order several times. That is, run the first one. Then try to guess the median and run the second cell to confirm your guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rng.choice(101, np.random.choice([20, 21], size = 1), replace=True)\n",
    "print(\"The NumPy array is \\n\", x)\n",
    "print(\"It has \",x.size, \" elements.\")\n",
    "print(\"The sorted array is \\n\", np.sort(x))\n",
    "if(x.size % 2 == 0):\n",
    "    print(\"Even case\")\n",
    "    print(\"The two central values are: \", np.sort(x)[9:11])\n",
    "else:\n",
    "    print(\"Odd case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The median is:\",  np.median(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88356236",
   "metadata": {},
   "source": [
    "+ Now let us check that the median is indeed robust to outliers. We repeat the same experiment as before, but now we compare the behavior of the median and the mean in the presence of an outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(rng.choice(101, 99, replace=True))\n",
    "print(\"Mean of x before appending = \", x.mean())\n",
    "print(\"Median of x before appending = \", np.median(x))\n",
    "x = np.append(x, 1000)\n",
    "print(\"Mean of x after appending = \", x.mean())\n",
    "print(\"Median of x after appending = \", np.median(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436c4d3",
   "metadata": {},
   "source": [
    "+ **Exercise:** Run the previous cell many times to see the difference between the median and the mean. Replace the outlier with an even bigger one and repeat the experiment.\n",
    "\n",
    "+ If the median is robust and the mean is not, why would anyone use the mean instead of the median? That is because the *mathematics of the median* are more complicated than those of the mean. It is harder to get theoretical results about the median, whereas the mean is simpler to work with. With the use of computers the median-based methods have become easier to apply, but the theoretical foundation remains harder. In practice we will often use the median as a way to gauge the influence of outliers in our work. If you have a sample where the mean and median are very similar that indicates that outliers are probably not affecting your conclusions. But if there is a big discrepancy between both values then we should start worrying about outliers and we must develop a strategy to deal with them. We will return to the management of outliers in the next sessions of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751b194",
   "metadata": {},
   "source": [
    "## Other position values: quartiles and percentiles.\n",
    "\n",
    "+ The median indicates the central position in our data: 50% of the data is smaller than the median and 50% is bigger. When we think of the median that way it is obvious that we can generalize the idea by considering percent values other than 50%. For example, the **quartiles** divide the (sorted) data in four parts. The *first quartile* divides the data in the smallest 25% and the rest. Similarly, the *third quartile* marks the boundary for the 75% smallest fraction of the data. And the *second quartile* is just another fancy name for the median. \n",
    "\n",
    "+ Once we get to that point, it is easy that we may consider any other percent value and use it to define the *percentiles*. In Python we can get any percentile with the NumPy `quantile` function (note there is a different letter in quaNtile vs quaRtile). We just need to pass a list of the percent values we want to use. For example to get the third quartiles we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595eb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2022)\n",
    "vals = rng.standard_normal(4000)\n",
    "vals[0:20]\n",
    "np.quantile(vals, [0.25, 0.5, 0.75])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3c2f7",
   "metadata": {},
   "source": [
    "### Max, min and range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833eb0a",
   "metadata": {},
   "source": [
    "+ The 0% and 100% percentiles are just the minimum and maximum values in the data. But of course we can get them with the NumPy methods `min` and `max`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddde4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Min = \", vals.min(), \"\\n Max = \", vals.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51b7e4",
   "metadata": {},
   "source": [
    "+ The `range` refers to the interval between the min and max of the data. Be careful: depending on who you are talking to, or the software you use, `range` can be used to talk about the *width* of the interval (`Max - Min`) or the actual interval `[Min, Max]`. \n",
    "\n",
    "+ In NumPy the first version of the range (the width, a single number) can be obtained with the `ptp` method (poor choice of named, coming from *peak to peak*).  \n",
    "  **Note:** for the Spanish speakers, *range* should always be translated to *recorrido*. You will often see it translated as *rango*, but that is just misleading (even though it is admitted). There is a another notion in Statistics called *rank* in English which is the one that needs to be translated as *rango*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac3d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Range = \", vals.ptp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb063b",
   "metadata": {},
   "source": [
    "+ The **interquartilic range (iqr)** is defined as the difference (rarely also as the interval) betqeen the third and first quartiles. That is, it is the *size of an interval containing the central 50% of our data*. We can get it in Python using `scipy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "print(stats.iqr(vals))\n",
    "# Uncomment the following line to check\n",
    "# print(np.quantile(vals, 0.75) - np.quantile(vals, 0.25)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83508320",
   "metadata": {},
   "source": [
    "## Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d366c",
   "metadata": {},
   "source": [
    "+ Look at the two data sets in the picture below, labeled `a` (in blue) and `b`(in red). The value for each data point is the horizontal coordinate, but the data have been randomly shifted to avoid overlapping and thus improve the visualization (this is called *jitter*). The number of data points is the same, their means are quite similar and the range of both samples is the same. But they look very different, because of the way the data are *spread* around the center. In the `a` sample most of the data are close to the center, with only a small portion of the data spreading far way to the left and right ends of the range. On the other hand, the data in `b` are much more evenly spread all through the range, in a *uniform* way. \n",
    "\n",
    "+ By the way, this kind of plot where we can see individual points (often using jitter) is called a *stripplot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145cc6b8",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "n = 400\n",
    "A = np.array(np.random.normal(size = n))\n",
    "minA = A.min()\n",
    "maxA = A.max()\n",
    "#A[0:10] = np.random.uniform(low = 2 * minA, high=2 * maxA, size = 10)\n",
    "B = np.array(np.random.uniform(low = 2 * minA, high=2 * maxA, size = n))\n",
    "A[0:2] = np.array([B.min(), B.max()])\n",
    "A = np.hstack((A, B))\n",
    "F = np.hstack((np.full(shape = (n,), fill_value='a'),\n",
    "               np.full(shape = (n,), fill_value='b')))\n",
    "# print(A.mean(), B.mean())\n",
    "\n",
    "X3 = pd.DataFrame({'A':A, 'F':F})\n",
    "#X3.head()\n",
    "\n",
    "ax = sns.stripplot(y=\"F\", x=\"A\", data=X3, jitter=0.05)\n",
    "getPlot = ax.set(xlabel=\"\", ylabel=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6d901",
   "metadata": {},
   "source": [
    "+ Another way to look at the idea of spread is using density curves. Again in this case the non-zero values of density are spread more widely for the red curve. Recall that the height in this curves indicates the number of values in that region. Thus the density curves carry the same  message as the stripplot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21f782",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "getPlot = sns.displot(X3, x=\"A\", kind=\"kde\", hue = \"F\", height=7, bw_adjust=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94322c9",
   "metadata": {},
   "source": [
    "+ How do we define *spread*, in order to measure it? Just as in the case of the central values, there are several options available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296771d9",
   "metadata": {},
   "source": [
    "### Median Absolute Deviation\n",
    "\n",
    "+ Let us consider a sample of $n$ values from a numeric variable $X$: \n",
    "$$x_1, x_2, \\ldots, x_n$$\n",
    "If $m$ is the median of this sample, the **deviation** of a point $x_i$ is simply the distance from that point to the median:\n",
    "$$d_i = \\left|x_i - m\\right|$$\n",
    "By computing the deviation for each value in the sample we get a new set of numbers (deviations)\n",
    "$$d_1, d_2, \\ldots, d_n$$\n",
    "and then we can define the **median absolute deviation (mad)** as the median of these distances:\n",
    "$$mad(x_1, x_2, \\ldots, x_n) = \\operatorname{median}\\left\\{\\left|x_i - m\\right|\\right\\}_{i = 1}^n$$\n",
    "The reason we are considering distances (by taking the absolute value) is that we never want negative deviations compensating for positive deviations when measuring the spread.\n",
    "\n",
    "+ The median absolute deviation, much as the median, is robust to outliers but it also leads to more complicated computation and much more complicated theoretical results. Therefore we turn to the classical alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac05c5",
   "metadata": {},
   "source": [
    "### Variance and Standard Deviation\n",
    "\n",
    "+ The first problem with the median absolute deviation is the use of the absolute value to remove the sign of the deviation. Ans that is a problem because the absolute value is not a derivable function at the origin. We can get rid of that problem by taking the square of the deviations:\n",
    "$$d_i^2 = \\left(\\left|x_i - m\\right|\\right)^2 = \\left(x_i - m\\right)^2$$\n",
    "The square removes the sign (and the absolute value) and it is a derivable function. But the problem is that now we are looking at squared deviations which, for starters, are not measured in the same units as the original data. Therefore we will need to take a square root at some point to get back to the original units.\n",
    "+ But before we do that we need to choose a representative or summary measure of all this individual spreads. We could take the median of the squared deviations, but then derivability would be gone again. Therefore we take the arithmetic mean of the squared deviations and call it the **(population) variance**:\n",
    "$$\\sigma^2_X = \\dfrac{(x_1 - \\bar X)^2 + (x_2 - \\bar X)^2 + \\cdots + (x_n - \\bar X)^2}{n} = \\dfrac{1}{n}\\sum_{i=1}^n (x_i - \\bar X)^2$$\n",
    "+ To get back to the original units we take then the square root of the variance and call it the **(population) standard deviation:**\n",
    "$$\\sigma_X = \\sqrt{\\dfrac{(x_1 - \\bar X)^2 + (x_2 - \\bar X)^2 + \\cdots + (x_n - \\bar X)^2}{n}} = \\sqrt{\\dfrac{1}{n}\\sum_{i=1}^n (x_i - \\bar X)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beebebd",
   "metadata": {},
   "source": [
    "### Population vs Sample Measures\n",
    "\n",
    "+ In the last paragraphs we have defined the *population* variance and standard deviation, which were intended to describe the spread of the variable. You may have also noticed that we did no effort to compute them using Python. The reason behind this will become more and more clear in the next sessions of the course. But it ultimately has to do with the **main goal of Statistics: to exract information about a population using samples drawn from that \n",
    "population.**\n",
    "\n",
    "+ In particular, we must accept that we will usually not have access to the whole data in the population. We wouldn't be doing Statistics in that case! And so we will be computing quantities from sample data and trying to use those quantities to **estimate unknown values  in the population**. \n",
    "\n",
    "+ For example, when we try to estimate the variance of a given variable in the population we will draw a sample. And then we need to use the data from that sample to obtain a numeric estimate, our best guess, of that *population variance*. The statisticians discovered long ago that if you apply to that sample the formulas from the previous section, dividing by $n$ (the size of the sample) then your guess of the population variance will tend to be off. The formulas need to be corrected if our sample-based guess of the variance is going to be a good estimator of the population variance. The correction, however, is not too big. The **sample variance** is defined simply changing $n$ by $n - 1$ in the denominator (in boldface below):\n",
    "$$s^2_X = \\dfrac{(x_1 - \\bar X)^2 + (x_2 - \\bar X)^2 + \\cdots + (x_n - \\bar X)^2}{\\mathbf{n - 1}} = \\dfrac{1}{\\mathbf n - 1}\\sum_{i=1}^n (x_i - \\bar X)^2$$\n",
    "+ And the corresponding the **sample standard deviation** is:\n",
    "$$s_X = \\sqrt{\\dfrac{(x_1 - \\bar X)^2 + (x_2 - \\bar X)^2 + \\cdots + (x_n - \\bar X)^2}{\\mathbf n - 1}} = \\sqrt{\\dfrac{1}{\\mathbf n - 1}\\sum_{i=1}^n (x_i - \\bar X)^2}$$\n",
    "Note that for the population definitions we used the Greek letter sigma $\\sigma$ while we use the letter $s$ for the sample quantities. \n",
    "\n",
    "+ In Python we can get the (sample) standard deviation using the `std` method in `pandas`. For example let us find the standard deviation of the `Speed` variable in the `train_accidents` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample variance of the speed: \", train_accidents['Speed'].var())\n",
    "print(\"and sample standard deviation: \", train_accidents['Speed'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc44c8",
   "metadata": {},
   "source": [
    "+ The sample formulas are the ones we will be using most of the time (after all, our starting point will always be a sample). But if you ever need to apply the population formulas you can do so with the `ddof` argument of the previous formulas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample variance of the speed: \", train_accidents['Speed'].var(ddof = 0))\n",
    "print(\"and sample standard deviation: \", train_accidents['Speed'].std(ddof = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e9fa7",
   "metadata": {},
   "source": [
    "In samples of this size the difference between the two quantities is really small, but it becomes more and more relevant as the sample size decreases. That is a constant theme in Statistics: as the sample size decreases the quality of the information does the same and the computation must take that into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8745d",
   "metadata": {},
   "source": [
    "## The Pandas describe function\n",
    "\n",
    "+ When we start exploring a new data set one of the first things we will do is to get some initial information about the distributions of the numerical variables in the data. We will use plots, but we in particular we will also get for each variable the common center, position and spread values we have been discussing. In the most common scenario, where the data is stored in a pandas DataFrame, there is a convenient function aptly called `describe` that will return many of those center and position values for all the numeric variables. You can force it to consider also factors, but the output in that case is not nearly as useful, so we will not discuss it here.\n",
    "\n",
    "+ Let us see how it looks like for the `train_accidents` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d239c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accidents.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096fbde",
   "metadata": {},
   "source": [
    "## Boxplots and the definition of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed70ec",
   "metadata": {},
   "source": [
    "+ Boxplots are a popular form of graphical representation of the position measures of a set of numeric values. They are also called *box and whiskers* plots for reasons that will become immediately aparent. The structure of a boxplot is illustrated in the following picture:\n",
    "\n",
    "![](fig/02-fig05-BoxPlotEstructura.png)\n",
    "\n",
    "As you can see one axis (in this case the vertical one) is chosen to represent the scale of your variable. The central 50% of the values is represented by a rectangular box (the width of the box in the other dimension is irrelevant). The bottom and top of the box correspond to the first and third quartiles, while the thick line inside the box indicates the position of the median. The segments coming out of the box are the so called *whiskers*. They connect the box with the more extreme non-outliers in your data. The outliers themselves, if they exist, are represented as dots disconnected from the box. \n",
    "\n",
    "+ **Definition of Outlier:** The previous discussion assumes that we know how to tell if a value is or not an outlier. There are many possible notions of outlier, but the most popular one and which is used to draw this boxplot is as follows: a value is an **outlier** if the distance from that value to the closest side of the box is bigger than 1.5 times the iqr.\n",
    "\n",
    "+ To draw a boxplot of the values of a column in a pandas DataFrame we can use the `box` method as follows. Here we look at the `price` variable in the `auto2` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c24b7b",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# getPlot = auto2['price'].plot.box(figsize=(10, 8))\n",
    "getPlot = auto2.boxplot(column='price', figsize=(6, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef709b",
   "metadata": {},
   "source": [
    "+ If you want to locate the outliers in this variable you can proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96bb1e5",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "price_iqr = stats.iqr(auto2.price)\n",
    "price_q1, price_q3 = np.quantile(auto2.price, [0.25, 0.75])\n",
    "\n",
    "cut_low = price_q1 - 1.5 * price_iqr\n",
    "cut_high = price_q3 + 1.5 * price_iqr\n",
    "\n",
    "print(\"Lower and upper outlier limits:\", cut_low, \", \", cut_high)\n",
    "\n",
    "price_outliers = auto2.loc[(auto2.price < cut_low) | (auto2.price > cut_high)]\n",
    "\n",
    "print(\"The outliers are\")\n",
    "\n",
    "price_outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb4144",
   "metadata": {},
   "source": [
    "+ Sometimes we will we interested in getting the boxplots for many numeric variables in the table at once. The code below shows how to do that for some of the variables in `auto2`.  \n",
    "**IMPORTANT:** remember to check if you use different scale axis for variables that measure different quantities. In such case the `sharex` or `sharey` arguments become essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c304fb0",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "ax = auto2.loc[:, [\"price\",\"mpg\", \"weight\", \"length\"]].plot.box(\n",
    "    subplots=True, layout=(2,2), sharex=False, sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007452a",
   "metadata": {},
   "source": [
    "+ **Violinplots** are a good complement to boxplots. They combine two symmetrical copies of the density curve with a boxplot, and thus overcome some of the limitations of a boxplot. For exampe, if the density curve is bimodal it is often hard to detect that in the boxplot, but the density curve will let us see that. For the `price` variable in `auto2` we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed32b9",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x = 'price', data=auto2, width=0.3, alpha=0.03, color = \"tan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc5d28",
   "metadata": {},
   "source": [
    "+ The default version of the violinplot in `seaborn` draws a mini-boxplot. If you would like to make it bigger you can remove that mini-box (with `inner=None`) and add a regular boxplot (playing with transparency and colors to get what you want, see [this link](https://stackoverflow.com/questions/62597959/seaborn-violinplot-transparency)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26609f96",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x = 'price', data=auto2, width=0.5, color=\"orange\", boxprops=dict(alpha=.3))\n",
    "ax = sns.violinplot(x = 'price', data=auto2, width=0.3, color = \"tan\", inner=None)\n",
    "for violin, alpha in zip(ax.collections[::2], [0.5]):\n",
    "    violin.set_alpha(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc858f",
   "metadata": {},
   "source": [
    "# Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a19559",
   "metadata": {},
   "source": [
    "+ *Factors*, also called *qualitative*, *categorical* or *class* variables are used mainly to create groups or categories of observations. Many of the data sets that we have seen or will see include factors. And recognizing that a variable should be considered as a factor is a key step in a well designed data science process. \n",
    "\n",
    "+ When we tell Python to treat a variable as a factor it will not only save some memory. Much more importantly, many methods in Statistics or Machine Learning behave differently when the input variable is a factor. Therefore we must always take some time in the exploratory analysis of the data to detect factors and let Python be aware of them.\n",
    "\n",
    "+ Quite often, when you import data from an external source you will find that some factors have been coded as a different data type (e.g. integers or `objects`). For example let us recall the data types in the `train_accidents` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accidents.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae733aa",
   "metadata": {},
   "source": [
    "+ Let us focus in two columns, `TrkType` and `AccType`.  Their absolute frequency tables are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accidents.TrkType.value_counts().sort_index(), \"\\n\"*3, \n",
    "      train_accidents.AccType.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f118be0",
   "metadata": {},
   "source": [
    "+ Think about these variables. Even their names are telling us that they divide the data into different classes or types. Therefore we **must** treat them as factors, otherwise our analysis will miss some useful information an will not be able to benefit from all the power of the statistical methods. \n",
    "+ Converting a few columns of a DataFrame to factors is easy with the `astype` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ef5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accidents = train_accidents.astype({'TrkType':'category', \n",
    "                                          'AccType':'category'})\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'a': [1, 2] * 3,\n",
    "                   'b': [True, False] * 3,\n",
    "                   'c1': [1.0, 2.0] * 3,\n",
    "                   'c2': [1.0, 2.0] * 3,\n",
    "                   'd': ['A', 'B'] * 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f3e10",
   "metadata": {},
   "source": [
    "+ If we now look at the data types we see that these two variables are now factors (of `category` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accidents.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd2f1c",
   "metadata": {},
   "source": [
    "+ You can learn more about [factors in pandas in this link](https://pandas.pydata.org/docs/user_guide/categorical.html). And we will learn through practice how to deal with them in the next sessions.\n",
    "\n",
    "+ The above method works fine if you only need to change a few columns to factor type. But if you need to batch convert a lot of columns then you can do so using the `select_dtypes` to select only those columns with `object` type. Note that you may have to exclude the purely character string columns before the conversion into factors.  The following example does this for the movies data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67601ff7",
   "metadata": {},
   "source": [
    "+ We want to convert `Genre` and `Lead Studio` to factors, but not `Film` (which is a title) or `Worlwide Gross` that is really numeric but needs a special treatment (we'll see about that later in the course). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcfd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce30090",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorCols = movies.select_dtypes(include='object').iloc[:, 1:3].columns\n",
    "print(factorCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[:, factorCols] = movies.loc[:, factorCols].astype('category')\n",
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d95122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e16e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8ffa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 202.79999387264252,
   "position": {
    "height": "78.4000015258789px",
    "left": "1167px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
